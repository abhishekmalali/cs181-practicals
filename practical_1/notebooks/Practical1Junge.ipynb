{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "# from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## To avoid using up submissions, we create our own test set out of the training data.  This allows us to experiment\n",
    "##    wildly and estimate error on various models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training Data...\n",
      "Splitting Data...\n"
     ]
    }
   ],
   "source": [
    "from harness import RMSE, train_test, write_to_file\n",
    "xtrain, xtest, ytrain, ytest=train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>feat_001</th>\n",
       "      <th>feat_002</th>\n",
       "      <th>feat_003</th>\n",
       "      <th>feat_004</th>\n",
       "      <th>feat_005</th>\n",
       "      <th>feat_006</th>\n",
       "      <th>feat_007</th>\n",
       "      <th>feat_008</th>\n",
       "      <th>feat_009</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_247</th>\n",
       "      <th>feat_248</th>\n",
       "      <th>feat_249</th>\n",
       "      <th>feat_250</th>\n",
       "      <th>feat_251</th>\n",
       "      <th>feat_252</th>\n",
       "      <th>feat_253</th>\n",
       "      <th>feat_254</th>\n",
       "      <th>feat_255</th>\n",
       "      <th>feat_256</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>276826</th>\n",
       "      <td>[nH]1cccc1-c1ccc(cn1)-c1sc(-c2nccs2)c2cc[nH]c12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849425</th>\n",
       "      <td>c1sc(-c2ccc(cc2)-c2sc(-c3cccc4c[nH]cc34)c3[se]...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504499</th>\n",
       "      <td>c1sc(-c2cc3[se]c4c(oc5ccc6cscc6c45)c3cn2)c2cc[...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601054</th>\n",
       "      <td>[nH]1c2C=C[SiH2]c2c2[nH]c3cc([se]c3c12)-c1cccc...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980221</th>\n",
       "      <td>[nH]1c(cc2c3cocc3c3c4ccccc4sc3c12)-c1cccc2cocc12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   smiles  feat_001  feat_002  \\\n",
       "276826    [nH]1cccc1-c1ccc(cn1)-c1sc(-c2nccs2)c2cc[nH]c12         0         0   \n",
       "849425  c1sc(-c2ccc(cc2)-c2sc(-c3cccc4c[nH]cc34)c3[se]...         0         0   \n",
       "504499  c1sc(-c2cc3[se]c4c(oc5ccc6cscc6c45)c3cn2)c2cc[...         0         0   \n",
       "601054  [nH]1c2C=C[SiH2]c2c2[nH]c3cc([se]c3c12)-c1cccc...         1         0   \n",
       "980221   [nH]1c(cc2c3cocc3c3c4ccccc4sc3c12)-c1cccc2cocc12         0         0   \n",
       "\n",
       "        feat_003  feat_004  feat_005  feat_006  feat_007  feat_008  feat_009  \\\n",
       "276826         0         0         1         1         1         0         0   \n",
       "849425         0         0         1         1         1         0         0   \n",
       "504499         0         0         1         1         1         0         0   \n",
       "601054         0         0         1         1         1         0         0   \n",
       "980221         0         0         1         1         1         0         0   \n",
       "\n",
       "          ...     feat_247  feat_248  feat_249  feat_250  feat_251  feat_252  \\\n",
       "276826    ...            0         1         0         0         0         0   \n",
       "849425    ...            0         1         0         0         0         0   \n",
       "504499    ...            0         1         0         0         0         0   \n",
       "601054    ...            0         0         0         0         0         0   \n",
       "980221    ...            0         1         0         0         0         0   \n",
       "\n",
       "        feat_253  feat_254  feat_255  feat_256  \n",
       "276826         0         0         0         0  \n",
       "849425         0         0         0         0  \n",
       "504499         0         0         0         0  \n",
       "601054         0         0         0         0  \n",
       "980221         0         0         0         0  \n",
       "\n",
       "[5 rows x 257 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtest_smiles = xtest['smiles']\n",
    "xtest = xtest.drop(['smiles'], axis=1)\n",
    "xtrain_smiles = xtrain['smiles']\n",
    "xtrain = xtrain.drop(['smiles'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExample Feature Engineering\\n\\nthis calculates the length of each smile string and adds a feature column with those lengths\\nNote: this is NOT a good feature and will result in a lower score!\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example Feature Engineering\n",
    "\n",
    "this calculates the length of each smile string and adds a feature column with those lengths\n",
    "Note: this is NOT a good feature and will result in a lower score!\n",
    "\"\"\"\n",
    "#smiles_len = np.vstack(df_all.smiles.astype(str).apply(lambda x: len(x)))\n",
    "#df_all['smiles_len'] = pd.DataFrame(smiles_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 2.85 s, total: 14.7 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "LR1 = LinearRegression()\n",
    "LR1.fit(xtrain, ytrain)\n",
    "# Prediction of LR on our test set\n",
    "LR_pred1 = LR1.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  0.298502133902\n"
     ]
    }
   ],
   "source": [
    "rmse1 = RMSE(LR_pred1, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.8 s, sys: 844 ms, total: 31.6 s\n",
      "Wall time: 31.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RF1 = RandomForestRegressor()\n",
    "RF1.fit(xtrain, ytrain)\n",
    "# Prediction of RF on the training set\n",
    "RF_pred1 = RF1.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  0.271381164183\n"
     ]
    }
   ],
   "source": [
    "rmse2 = RMSE(RF_pred1, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## In the provided code, the RF predictor performs better than LR on the training set, but the \n",
    "##     LR predictor performs better than RF on the actual uploaded test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## We will use these error values as baselines for our model tweaks.\n",
    "def LRimprovement(prediction, test=ytest):\n",
    "    imp = rmse1 - RMSE(prediction, test)\n",
    "    print \"Improvement on original Linear Regression: \", imp\n",
    "    return imp\n",
    "\n",
    "def RFimprovement(prediction, test=ytest):\n",
    "    imp = rmse2 - RMSE(prediction, test)\n",
    "    print \"Improvement on original Random Forest: \", imp\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## To beat the baseline, we will want to do some exploratory analysis to get some intuition on the features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum gap in training set:  3.8\n",
      "Maximum gap in test set:      3.43\n",
      "Minimum gap in training set:  -1.44\n",
      "Minimum gap in test set:      0.75\n"
     ]
    }
   ],
   "source": [
    "print \"Maximum gap in training set: \", np.max(ytrain)\n",
    "print \"Maximum gap in test set:     \", np.max(ytest)\n",
    "print \"Minimum gap in training set: \", np.min(ytrain)\n",
    "print \"Minimum gap in test set:     \", np.min(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Notice the anomalous value in the training set.  Let's look at the distribution of gap values in each set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2, 4)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x171c91a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ytest, bins=np.linspace(-2,4,20))\n",
    "plt.xlim([-2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2, 4)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJDUZFJKkJoNCktRkUEiSmgwKSVLT/wcs97Vj0PnltwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1156d2650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ytrain, bins=np.linspace(-2,4,20))\n",
    "plt.xlim([-2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The distributions are the same shape, but there are three values in the test set that are clearly erroneous:\n",
    "np.sum(ytrain<0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Let's drop those three rows out, because they can't be helping our models:\n",
    "xtrain = xtrain[ytrain>0]\n",
    "ytrain = ytrain[ytrain>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  0.298501403843\n",
      "Improvement on original Linear Regression:  7.30058257015e-07\n",
      "CPU times: user 13.4 s, sys: 3.73 s, total: 17.1 s\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "LR = LinearRegression()\n",
    "LR.fit(xtrain, ytrain)\n",
    "# Prediction of LR on our test set\n",
    "LR_pred = LR.predict(xtest)\n",
    "LRimprovement(LR_pred)\n",
    "# RMSE(LR_pred, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  0.271364949307\n",
      "Improvement on original Random Forest:  1.62148765907e-05\n",
      "CPU times: user 30.7 s, sys: 767 ms, total: 31.5 s\n",
      "Wall time: 31.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RF = RandomForestRegressor()\n",
    "RF.fit(xtrain, ytrain)\n",
    "# Prediction of RF on the training set\n",
    "RF_pred = RF.predict(xtest)\n",
    "RFimprovement(RF_pred)\n",
    "# RMSE(RF_pred, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This didn't make much difference, but there is no reason to keep the rows with implausible gap values in our \n",
    "#      training set, so we will stick with this.  \n",
    "\n",
    "## Next, let's take a look at the features themselves.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_001</th>\n",
       "      <th>feat_002</th>\n",
       "      <th>feat_003</th>\n",
       "      <th>feat_004</th>\n",
       "      <th>feat_005</th>\n",
       "      <th>feat_006</th>\n",
       "      <th>feat_007</th>\n",
       "      <th>feat_008</th>\n",
       "      <th>feat_009</th>\n",
       "      <th>feat_010</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_247</th>\n",
       "      <th>feat_248</th>\n",
       "      <th>feat_249</th>\n",
       "      <th>feat_250</th>\n",
       "      <th>feat_251</th>\n",
       "      <th>feat_252</th>\n",
       "      <th>feat_253</th>\n",
       "      <th>feat_254</th>\n",
       "      <th>feat_255</th>\n",
       "      <th>feat_256</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>899997.000000</td>\n",
       "      <td>899997</td>\n",
       "      <td>899997</td>\n",
       "      <td>899997</td>\n",
       "      <td>899997.000000</td>\n",
       "      <td>899997.000000</td>\n",
       "      <td>899997.000000</td>\n",
       "      <td>899997</td>\n",
       "      <td>899997</td>\n",
       "      <td>899997</td>\n",
       "      <td>...</td>\n",
       "      <td>899997</td>\n",
       "      <td>899997.000000</td>\n",
       "      <td>899997</td>\n",
       "      <td>899997</td>\n",
       "      <td>899997.000000</td>\n",
       "      <td>899997.000000</td>\n",
       "      <td>899997</td>\n",
       "      <td>899997</td>\n",
       "      <td>899997</td>\n",
       "      <td>899997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.641930</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>0.477699</td>\n",
       "      <td>0.975871</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.917416</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.218753</td>\n",
       "      <td>0.037247</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.479433</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003801</td>\n",
       "      <td>0.499503</td>\n",
       "      <td>0.153450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.275252</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.413401</td>\n",
       "      <td>0.189366</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            feat_001  feat_002  feat_003  feat_004       feat_005  \\\n",
       "count  899997.000000    899997    899997    899997  899997.000000   \n",
       "mean        0.641930         0         0         0       0.999986   \n",
       "std         0.479433         0         0         0       0.003801   \n",
       "min         0.000000         0         0         0       0.000000   \n",
       "25%         0.000000         0         0         0       1.000000   \n",
       "50%         1.000000         0         0         0       1.000000   \n",
       "75%         1.000000         0         0         0       1.000000   \n",
       "max         1.000000         0         0         0       1.000000   \n",
       "\n",
       "            feat_006       feat_007  feat_008  feat_009  feat_010    ...     \\\n",
       "count  899997.000000  899997.000000    899997    899997    899997    ...      \n",
       "mean        0.477699       0.975871         0         0         0    ...      \n",
       "std         0.499503       0.153450         0         0         0    ...      \n",
       "min         0.000000       0.000000         0         0         0    ...      \n",
       "25%         0.000000       1.000000         0         0         0    ...      \n",
       "50%         0.000000       1.000000         0         0         0    ...      \n",
       "75%         1.000000       1.000000         0         0         0    ...      \n",
       "max         1.000000       1.000000         0         0         0    ...      \n",
       "\n",
       "       feat_247       feat_248  feat_249  feat_250       feat_251  \\\n",
       "count    899997  899997.000000    899997    899997  899997.000000   \n",
       "mean          0       0.917416         0         0       0.218753   \n",
       "std           0       0.275252         0         0       0.413401   \n",
       "min           0       0.000000         0         0       0.000000   \n",
       "25%           0       1.000000         0         0       0.000000   \n",
       "50%           0       1.000000         0         0       0.000000   \n",
       "75%           0       1.000000         0         0       0.000000   \n",
       "max           0       1.000000         0         0       1.000000   \n",
       "\n",
       "            feat_252  feat_253  feat_254  feat_255  feat_256  \n",
       "count  899997.000000    899997    899997    899997    899997  \n",
       "mean        0.037247         0         0         0         0  \n",
       "std         0.189366         0         0         0         0  \n",
       "min         0.000000         0         0         0         0  \n",
       "25%         0.000000         0         0         0         0  \n",
       "50%         0.000000         0         0         0         0  \n",
       "75%         0.000000         0         0         0         0  \n",
       "max         1.000000         0         0         0         0  \n",
       "\n",
       "[8 rows x 256 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_descrip = xtrain.describe()\n",
    "## Notice that some features have 0 values for all observations.  These columns contain no information, so perhaps\n",
    "##     we should remove these from the model.  We could remove these columns from the training set, train the model, \n",
    "##     then remove the same columns from the test set before making the predictions.  \n",
    "train_descrip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## In fact, 225 features have zero mean\n",
    "sum(train_descrip.loc['mean']==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## But none have a mean of 1\n",
    "sum(train_descrip.loc['mean']==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## There are 4 features that apply to less than 5% of the samples.  Maybe some of these would be worth removing \n",
    "##     as well.  We will try a few strategies for removing features that we think will not be meaningful.  \n",
    "sum(train_descrip.loc['mean']>=(0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Names of all the features that are all zeroes in the training set\n",
    "zeros = train_descrip.columns.values[np.array(train_descrip.loc['mean']==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop the columns that are filled with all zeroes\n",
    "xtrain = xtrain.drop(zeros, axis=1)\n",
    "xtest = xtest.drop(zeros, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  0.298501008236\n",
      "Improvement on original Linear Regression:  1.12566556459e-06\n",
      "CPU times: user 1.66 s, sys: 281 ms, total: 1.94 s\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fit a new linear regression on just these columns.  Note that the error is nearly identical.  This makes sense\n",
    "#    because presumably the coefficients for those columns are zero, or the values for those columns in the \n",
    "#    validation set are still zero.  \n",
    "LR = LinearRegression()\n",
    "LR.fit(xtrain, ytrain)\n",
    "LR_pred = LR.predict(xtest)\n",
    "LRimprovement(LR_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  0.271354984236\n",
      "Improvement on original Random Forest:  2.61799471774e-05\n",
      "CPU times: user 22.1 s, sys: 182 ms, total: 22.2 s\n",
      "Wall time: 22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Let's see if this pattern holds for the Random Forest Regression.  \n",
    "RF = RandomForestRegressor()\n",
    "RF.fit(xtrain, ytrain)\n",
    "# Prediction of RF on the training set\n",
    "RF_pred = RF.predict(xtest)\n",
    "RFimprovement(RF_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.21173353e-02,   1.81377490e-01,  -3.65069424e+08,\n",
       "         2.67643705e-01,  -5.96347809e-02,   2.24324275e-01,\n",
       "         3.27869629e-01,  -2.20768335e-01,  -2.88212524e-01,\n",
       "        -1.82282045e-02,  -6.23139020e-02,  -9.47358714e-02,\n",
       "        -1.44188097e+08,  -4.31476123e-01,  -2.96213518e-02,\n",
       "         1.82046149e-02,  -1.78602675e-01,   4.05439338e-02,\n",
       "        -2.12827565e-02,  -6.59596074e-02,  -3.12987580e-01,\n",
       "         9.91659977e+10,  -9.91659977e+10,   1.90318568e-01,\n",
       "         1.44188097e+08,  -1.36830584e-01,   1.43435332e-01,\n",
       "         3.65069424e+08,  -7.13867052e-02,  -3.18902486e-01,\n",
       "        -7.33442063e-02])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This didn't significantly improve the RMSE value of either model, but there is still no reason to keep those \n",
    "##    columns around as they are not informing any model that we develop.  \n",
    "\n",
    "## Look at the regression coefficients.  Notice that 6 of them have massive absolute values, on the order of 1e08-1e11.  \n",
    "##    Next, notice that the element in the 3rd and 28th position are exactly the opposite, as are the 13th and 25th,\n",
    "#     and 22nd and 23rd.  \n",
    "LR.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## We can show that these features have identical values, and the regression simply assigns coefficients to cancel\n",
    "##    each other out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(xtrain.iloc[:,21] - xtrain.iloc[:,22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(xtrain.iloc[:,27] - xtrain.iloc[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(xtrain.iloc[:,12] - xtrain.iloc[:,24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'feat_200', u'feat_006', u'feat_218'], dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## These are the names of the columns that have LR coefficients less than one.  This includes the features that are\n",
    "###    assigned low importance and the redundant features.\n",
    "dropcols = xtrain.columns[[22,2,24]]\n",
    "xtrain_reduced = xtrain.drop(dropcols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  0.298500632144\n",
      "Improvement on original Linear Regression:  1.5017577622e-06\n",
      "CPU times: user 1.53 s, sys: 268 ms, total: 1.8 s\n",
      "Wall time: 1.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Now we do the same column removal to the validation set\n",
    "xtest_reduced = xtest.drop(dropcols, axis=1)\n",
    "\n",
    "LR = LinearRegression()\n",
    "LR.fit(xtrain_reduced, ytrain)\n",
    "LR_pred = LR.predict(xtest_reduced)\n",
    "LRimprovement(LR_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  0.271369101421\n",
      "Improvement on original Random Forest:  1.20627623686e-05\n",
      "CPU times: user 20.9 s, sys: 194 ms, total: 21.1 s\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RF = RandomForestRegressor()\n",
    "RF.fit(xtrain_reduced, ytrain)\n",
    "RF_pred = RF.predict(xtest_reduced)\n",
    "RFimprovement(RF_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Next, we look at how PCA changes the performance of the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make two lists to store improvement values of the models on pca-transformed data\n",
    "lr_pca = [10] # 0th element has nonsense value\n",
    "rf_pca = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  0.400562973676\n",
      "Improvement on original Linear Regression:  -0.102060839775\n",
      "RMSE =  0.271489329173\n",
      "Improvement on original Random Forest:  -0.000108164990031\n",
      "RMSE =  0.400184814218\n",
      "Improvement on original Linear Regression:  -0.101682680316\n",
      "RMSE =  0.271408440308\n",
      "Improvement on original Random Forest:  -2.72761247125e-05\n",
      "RMSE =  0.376814983764\n",
      "Improvement on original Linear Regression:  -0.0783128498626\n",
      "RMSE =  0.271395940436\n",
      "Improvement on original Random Forest:  -1.47762523914e-05\n",
      "RMSE =  0.376750152263\n",
      "Improvement on original Linear Regression:  -0.0782480183617\n",
      "RMSE =  0.271371323217\n",
      "Improvement on original Random Forest:  9.8409656869e-06\n",
      "RMSE =  0.346639600485\n",
      "Improvement on original Linear Regression:  -0.0481374665835\n",
      "RMSE =  0.271363220534\n",
      "Improvement on original Random Forest:  1.79436487533e-05\n",
      "RMSE =  0.34650256105\n",
      "Improvement on original Linear Regression:  -0.048000427148\n",
      "RMSE =  0.271347458613\n",
      "Improvement on original Random Forest:  3.37055702034e-05\n",
      "RMSE =  0.321184300739\n",
      "Improvement on original Linear Regression:  -0.0226821668374\n",
      "RMSE =  0.271366411624\n",
      "Improvement on original Random Forest:  1.47525595568e-05\n",
      "RMSE =  0.320022917739\n",
      "Improvement on original Linear Regression:  -0.0215207838371\n",
      "RMSE =  0.271383107801\n",
      "Improvement on original Random Forest:  -1.94361752232e-06\n",
      "RMSE =  0.312247966754\n",
      "Improvement on original Linear Regression:  -0.0137458328525\n",
      "RMSE =  0.271350589165\n",
      "Improvement on original Random Forest:  3.05750182688e-05\n",
      "RMSE =  0.312233224186\n",
      "Improvement on original Linear Regression:  -0.0137310902847\n",
      "RMSE =  0.271362969079\n",
      "Improvement on original Random Forest:  1.81951041801e-05\n",
      "RMSE =  0.312039128043\n",
      "Improvement on original Linear Regression:  -0.0135369941416\n",
      "RMSE =  0.271381465579\n",
      "Improvement on original Random Forest:  -3.01395505276e-07\n",
      "RMSE =  0.311637932931\n",
      "Improvement on original Linear Regression:  -0.0131357990292\n",
      "RMSE =  0.271376163541\n",
      "Improvement on original Random Forest:  5.00064168657e-06\n",
      "RMSE =  0.311632849393\n",
      "Improvement on original Linear Regression:  -0.0131307154911\n",
      "RMSE =  0.271363467743\n",
      "Improvement on original Random Forest:  1.76964405805e-05\n",
      "RMSE =  0.310529867254\n",
      "Improvement on original Linear Regression:  -0.0120277333528\n",
      "RMSE =  0.271357990503\n",
      "Improvement on original Random Forest:  2.31736801375e-05\n",
      "RMSE =  0.305042930976\n",
      "Improvement on original Linear Regression:  -0.00654079707387\n",
      "RMSE =  0.271362146062\n",
      "Improvement on original Random Forest:  1.90181215255e-05\n",
      "RMSE =  0.304740934844\n",
      "Improvement on original Linear Regression:  -0.00623880094191\n",
      "RMSE =  0.271353010667\n",
      "Improvement on original Random Forest:  2.81535158408e-05\n",
      "RMSE =  0.302330512434\n",
      "Improvement on original Linear Regression:  -0.0038283785323\n",
      "RMSE =  0.271352674638\n",
      "Improvement on original Random Forest:  2.84895447671e-05\n",
      "RMSE =  0.301784107931\n",
      "Improvement on original Linear Regression:  -0.00328197402957\n",
      "RMSE =  0.271398714601\n",
      "Improvement on original Random Forest:  -1.75504181838e-05\n",
      "RMSE =  0.30157412896\n",
      "Improvement on original Linear Regression:  -0.00307199505793\n",
      "RMSE =  0.271342385687\n",
      "Improvement on original Random Forest:  3.87784959234e-05\n",
      "RMSE =  0.301555580741\n",
      "Improvement on original Linear Regression:  -0.00305344683896\n",
      "RMSE =  0.27136592197\n",
      "Improvement on original Random Forest:  1.52422130487e-05\n",
      "RMSE =  0.301554465131\n",
      "Improvement on original Linear Regression:  -0.00305233122905\n",
      "RMSE =  0.271352843326\n",
      "Improvement on original Random Forest:  2.83208575531e-05\n",
      "RMSE =  0.299947677456\n",
      "Improvement on original Linear Regression:  -0.00144554355412\n",
      "RMSE =  0.271372765886\n",
      "Improvement on original Random Forest:  8.39829693183e-06\n",
      "RMSE =  0.299194741997\n",
      "Improvement on original Linear Regression:  -0.000692608095255\n",
      "RMSE =  0.271347971428\n",
      "Improvement on original Random Forest:  3.31927548398e-05\n",
      "RMSE =  0.298889043326\n",
      "Improvement on original Linear Regression:  -0.000386909423892\n",
      "RMSE =  0.271339788042\n",
      "Improvement on original Random Forest:  4.13761414352e-05\n",
      "RMSE =  0.298869731851\n",
      "Improvement on original Linear Regression:  -0.00036759794911\n",
      "RMSE =  0.271374094854\n",
      "Improvement on original Random Forest:  7.06932879602e-06\n",
      "RMSE =  0.298496159881\n",
      "Improvement on original Linear Regression:  5.97402079155e-06\n",
      "RMSE =  0.271387168063\n",
      "Improvement on original Random Forest:  -6.00387971011e-06\n",
      "RMSE =  0.298500892931\n",
      "Improvement on original Linear Regression:  1.24097023441e-06\n",
      "RMSE =  0.271375345695\n",
      "Improvement on original Random Forest:  5.81848854675e-06\n",
      "RMSE =  0.298500632144\n",
      "Improvement on original Linear Regression:  1.50175776126e-06\n",
      "RMSE =  0.271376966594\n",
      "Improvement on original Random Forest:  4.1975894845e-06\n",
      "RMSE =  0.298500575822\n",
      "Improvement on original Linear Regression:  1.55807956526e-06\n",
      "RMSE =  0.271347179276\n",
      "Improvement on original Random Forest:  3.39849072014e-05\n",
      "RMSE =  0.298500554661\n",
      "Improvement on original Linear Regression:  1.57924037952e-06\n",
      "RMSE =  0.27135325563\n",
      "Improvement on original Random Forest:  2.79085529087e-05\n",
      "RMSE =  0.29850099239\n",
      "Improvement on original Linear Regression:  1.14151135672e-06\n",
      "RMSE =  0.271382127759\n",
      "Improvement on original Random Forest:  -9.63575537971e-07\n",
      "CPU times: user 27min 1s, sys: 21.7 s, total: 27min 23s\n",
      "Wall time: 27min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(1,32):\n",
    "    pca = PCA(n_components=i)\n",
    "    x_pca = pca.fit_transform(xtrain)\n",
    "    xtest_pca = pca.transform(xtest)\n",
    "    \n",
    "    LR = LinearRegression()\n",
    "    LR.fit(x_pca, ytrain)\n",
    "    LR_pred = LR.predict(xtest_pca)\n",
    "    lr_pca.append(LRimprovement(LR_pred))\n",
    "    \n",
    "    RF = RandomForestRegressor()\n",
    "    RF.fit(x_pca, ytrain)\n",
    "    RF_pred = RF.predict(xtest_pca)\n",
    "    rf_pca.append(RFimprovement(RF_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x131848350>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEACAYAAAC3adEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x184992f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_pca[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1848fabd0>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEACAYAAABhzAtFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njHPOuaL5/2xL8GVkupvXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x146e40d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rf_pca[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = df_train['smiles'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### From the sample notebook:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read in train and test as Pandas DataFrames\n",
    "\"\"\"\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>feat_001</th>\n",
       "      <th>feat_002</th>\n",
       "      <th>feat_003</th>\n",
       "      <th>feat_004</th>\n",
       "      <th>feat_005</th>\n",
       "      <th>feat_006</th>\n",
       "      <th>feat_007</th>\n",
       "      <th>feat_008</th>\n",
       "      <th>feat_009</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_247</th>\n",
       "      <th>feat_248</th>\n",
       "      <th>feat_249</th>\n",
       "      <th>feat_250</th>\n",
       "      <th>feat_251</th>\n",
       "      <th>feat_252</th>\n",
       "      <th>feat_253</th>\n",
       "      <th>feat_254</th>\n",
       "      <th>feat_255</th>\n",
       "      <th>feat_256</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c1ccc(o1)-c1ccc(s1)-c1cnc(-c2scc3[se]ccc23)c2n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1=CC=C(C1)c1cc2ncc3c4[SiH2]C=Cc4ncc3c2c2=C[Si...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[nH]1c-2c([SiH2]c3cc(-c4scc5C=CCc45)c4nsnc4c-2...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nH]1c2-c3occc3Cc2c2c1cc(-c1cccc3=C[SiH2]C=c13...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c1cnc2c3oc4cc(-c5ncncn5)c5nsnc5c4c3c3cocc3c2c1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  feat_001  feat_002  \\\n",
       "0  c1ccc(o1)-c1ccc(s1)-c1cnc(-c2scc3[se]ccc23)c2n...         0         0   \n",
       "1  C1=CC=C(C1)c1cc2ncc3c4[SiH2]C=Cc4ncc3c2c2=C[Si...         1         0   \n",
       "2  [nH]1c-2c([SiH2]c3cc(-c4scc5C=CCc45)c4nsnc4c-2...         1         0   \n",
       "3  [nH]1c2-c3occc3Cc2c2c1cc(-c1cccc3=C[SiH2]C=c13...         1         0   \n",
       "4     c1cnc2c3oc4cc(-c5ncncn5)c5nsnc5c4c3c3cocc3c2c1         0         0   \n",
       "\n",
       "   feat_003  feat_004  feat_005  feat_006  feat_007  feat_008  feat_009  \\\n",
       "0         0         0         1         0         1         0         0   \n",
       "1         0         0         1         0         1         0         0   \n",
       "2         0         0         1         1         1         0         0   \n",
       "3         0         0         1         1         1         0         0   \n",
       "4         0         0         1         0         1         0         0   \n",
       "\n",
       "     ...     feat_247  feat_248  feat_249  feat_250  feat_251  feat_252  \\\n",
       "0    ...            0         1         0         0         0         0   \n",
       "1    ...            0         1         0         0         1         0   \n",
       "2    ...            0         1         0         0         0         1   \n",
       "3    ...            0         1         0         0         0         1   \n",
       "4    ...            0         1         0         0         0         0   \n",
       "\n",
       "   feat_253  feat_254  feat_255  feat_256  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 257 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DataFrame with all train and test examples so we can more easily apply feature engineering on\n",
    "df_all = pd.concat((df_train, df_test), axis=0)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features: (1000000, 256)\n",
      "Train gap: (1000000,)\n",
      "Test features: (824230, 256)\n"
     ]
    }
   ],
   "source": [
    "#Drop the 'smiles' column\n",
    "df_all = df_all.drop(['smiles'], axis=1)\n",
    "vals = df_all.values\n",
    "X_train = vals[:test_idx]\n",
    "X_test = vals[test_idx:]\n",
    "print \"Train features:\", X_train.shape\n",
    "print \"Train gap:\", Y_train.shape\n",
    "print \"Test features:\", X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_file(\"sample1.csv\", LR_pred)\n",
    "write_to_file(\"sample2.csv\", RF_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#store gap values\n",
    "Y_train = df_train.gap.values\n",
    "#row where testing examples start\n",
    "test_idx = df_train.shape[0]\n",
    "#delete 'Id' column\n",
    "df_test = df_test.drop(['Id'], axis=1)\n",
    "#delete 'gap' column\n",
    "df_train = df_train.drop(['gap'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_to_file(filename, predictions):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"Id,Prediction\\n\")\n",
    "        for i,p in enumerate(predictions):\n",
    "            f.write(str(i+1) + \",\" + str(p) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
